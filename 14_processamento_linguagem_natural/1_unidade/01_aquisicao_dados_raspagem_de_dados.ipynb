{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Aquisição de dados com raspagem de dados**"
      ],
      "metadata": {
        "id": "R3tUTgb_kpwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação de pacotes para raspagem e análise de dados.\n",
        "!pip install beautifulsoup4 pandas requests"
      ],
      "metadata": {
        "id": "NB3Mmh26EKbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declara define funções utilitárias utilizadas no notebook.\n",
        "import datetime\n",
        "def formata_msg(nivel, msg):\n",
        "    \"\"\"\n",
        "    Formata uma mensagem de log incluindo o nível de severidade, timestamp\n",
        "    e a mensagem.\n",
        "\n",
        "    Parâmetros:\n",
        "    - nivel (str): Nível de severidade da mensagem (ex: 'INFO', 'ERROR', 'WARNING').\n",
        "    - msg (str): A mensagem de log propriamente dita.\n",
        "\n",
        "    Retorna:\n",
        "    - str: A mensagem de log formatada.\n",
        "    \"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    return f\"[{nivel}] {timestamp} - {msg}\""
      ],
      "metadata": {
        "id": "8ueIJQz8n-h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa módulos essenciais para funcionalidades do notebook.\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(formata_msg(\"INFO\", \"Bibliotecas importadas com sucesso; ambiente pronto.\"))"
      ],
      "metadata": {
        "id": "QUwhf4VtH9Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o diretório local para armazenar as bases de dados públicas coletadas.\n",
        "corpora_caminho = \"./corpora\"\n",
        "\n",
        "# Verifica se o diretório especificado já existe no ambiente do Colab.\n",
        "if not os.path.exists(corpora_caminho):\n",
        "  # Caso o diretório não exista, cria um novo diretório com o nome 'corpora'.\n",
        "  # Isso é útil para organizar os arquivos de dados baixados ou gerados.\n",
        "  print(formata_msg(\"INFO\", f\"Diretório {corpora_caminho} foi criado com sucesso.\"))\n",
        "  os.mkdir(corpora_caminho)\n",
        "else:\n",
        "  print(formata_msg(\"INFO\", f\"Diretório {corpora_caminho} já existe no sistema.\"))"
      ],
      "metadata": {
        "id": "Ld0Thaj9nqzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. G1"
      ],
      "metadata": {
        "id": "pzFkGfrRd65P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Raspagem de dados de notícias do G1"
      ],
      "metadata": {
        "id": "q6aSu0KT4paP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O G1 é um portal de notícias brasileiro mantido pelo Grupo Globo, sob a orientação da Central Globo de Jornalismo. Lançado em 18 de setembro de 2006, no ano em que a TV Globo completou 41 anos, o portal reúne conteúdos jornalísticos das diversas empresas do Grupo Globo — incluindo TV Globo, GloboNews, rádio CBN, os jornais O Globo, Extra, Valor Econômico, a revista Globo Rural, entre outros — além de produzir reportagens exclusivas em formatos de texto, fotos, áudio e vídeo.\n",
        "\n",
        "Os elementos estruturais da página principal, essenciais para a raspagem de dados, incluem as seguintes classes CSS:\n",
        "\n",
        "* **feed-post-body:** corpo da notícia;\n",
        "* **feed-post-link:** título da notícia;\n",
        "* **feed-post-body-resumo:** subtítulo da notícia (opcional).\n",
        "\n"
      ],
      "metadata": {
        "id": "c0tgNZXdnG07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVXqB01uDacX"
      },
      "outputs": [],
      "source": [
        "# Declara a URL do site para a raspagem de dados.\n",
        "url = \"https://g1.globo.com/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o nome do arquivo onde os dados serão salvos.\n",
        "noticias_arquivo = \"g1_noticias.csv\"\n",
        "\n",
        "# Usa 'os.path.join' para criar um caminho completo, garantindo compatibilidade\n",
        "# entre sistemas operacionais.\n",
        "noticias_caminho = os.path.join(corpora_caminho, noticias_arquivo)\n",
        "\n",
        "# Inicializa uma lista para armazenar os dados das notícias.\n",
        "noticias_g1 = []\n",
        "\n",
        "# Faz uma solicitação HTTP GET para a URL especificada.\n",
        "resposta = requests.get(url)\n",
        "\n",
        "# Verifica se a solicitação foi bem-sucedida (código 200).\n",
        "if resposta.status_code == 200:\n",
        "  # Processa o conteúdo da página se a solicitação for bem-sucedida.\n",
        "  pagina = resposta.content\n",
        "  soup = BeautifulSoup(pagina, 'html.parser')\n",
        "\n",
        "  # Encontra todos os elementos que correspondem ao corpo da notícia.\n",
        "  noticias = soup.findAll('div', attrs={'class': 'feed-post-body'})\n",
        "\n",
        "  # Itera sobre cada notícia encontrada.\n",
        "  for noticia in noticias:\n",
        "    # Extrai o título da notícia.\n",
        "    titulo = noticia.find('a', attrs={'class': 'feed-post-link'})\n",
        "\n",
        "    # Tenta encontrar o subtítulo da notícia.\n",
        "    subtitulo = noticia.find('div', attrs={'class': 'feed-post-body-resumo'})\n",
        "\n",
        "    # Adiciona o título, subtítulo (se houver) e o link à lista de notícias.\n",
        "    if (subtitulo):\n",
        "      noticias_g1.append([titulo.text, subtitulo.text, titulo['href']])\n",
        "    else:\n",
        "      noticias_g1.append([titulo.text, '', titulo['href']])\n",
        "\n",
        "  # Converte a lista de notícias em um DataFrame para facilitar a manipulação.\n",
        "  noticias_g1_df = pd.DataFrame(noticias_g1, columns=['titulo', 'subtitulo', 'link'])\n",
        "\n",
        "  # Salva os dados coletados no arquivo CSV especificado.\n",
        "  noticias_g1_df.to_csv(noticias_caminho, index=False)\n",
        "\n",
        "  print(formata_msg(\"INFO\", f\"Dados coletados com sucesso da URL: {url}.\"))\n",
        "  print(formata_msg(\"INFO\", f\"Os resultados foram salvos no arquivo: {noticias_arquivo}.\"))\n",
        "else:\n",
        "  print(\"Falha na solicitação, código de status:\", resposta.status_code)\n",
        "\n"
      ],
      "metadata": {
        "id": "woasdVNlm_gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega os dados do arquivo CSV especificado no DataFrame 'noticias_g1_df'.\n",
        "noticias_g1_df = pd.read_csv(noticias_caminho)\n",
        "noticias_g1_df.head(10)"
      ],
      "metadata": {
        "id": "57kIS4iOsTM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}